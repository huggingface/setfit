{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76571396-8f54-40ed-9e81-6c7531e6eaee",
   "metadata": {
    "id": "76571396-8f54-40ed-9e81-6c7531e6eaee"
   },
   "source": [
    "# Efficiently run SetFit Models with Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead5bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this notebook using `numactl`` to gain better control on which resources you are running. \n",
    "# It is best to run on a single socket.\n",
    "\n",
    "# NUM_THREADS=<NUM_AVAILABLE_CORES>\n",
    "# OMP_NUM_THREADS=$NUM_THREADS numactl -C 0-\"$(($NUM_THREADS-1))\" -m 0 jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e4e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install matplotlib -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55756fec-fc22-4590-84d7-2f3df37b9256",
   "metadata": {
    "id": "55756fec-fc22-4590-84d7-2f3df37b9256"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/sdp/anaconda3/envs/setfit-pr/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"]\n",
    "from setfit import SetFitModel\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, model, dataset, optim_type):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        self.enable_autocast = 'optimum' in self.optim_type.lower()\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        with torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "            preds = self.model.predict(self.dataset[\"text\"])\n",
    "        labels = self.dataset[\"label\"]\n",
    "        accuracy = metric.compute(predictions=preds, references=labels)\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "\n",
    "    def compute_size(self):\n",
    "        state_dict = self.model.model_body.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        # Calculate size in megabytes\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        # Delete temporary file\n",
    "        tmp_path.unlink()\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def compute_latency(self, query=\"that loves its characters and communicates something rather beautiful about human nature\"):\n",
    "        latencies = []\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "                _ = self.model([query])\n",
    "        # Timed run\n",
    "        for _ in range(200):\n",
    "            start_time = perf_counter()\n",
    "            with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "                _ = self.model([query])\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        # Compute run statistics\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(rf\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "    def compute_throughput(self, batch_sizes=[64, 512, 8192]):\n",
    "        results = {}\n",
    "        num_iters = 5\n",
    "        replications = int((max(batch_sizes) * (num_iters + 1)) / len(self.dataset))\n",
    "        replicated_ds = (self.dataset[\"text\"] * replications)[:max(batch_sizes) * num_iters]\n",
    "\n",
    "        s = \"\"\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Batch size: {batch_size}\")\n",
    "            \n",
    "            throughputs = []\n",
    "            dataloader = iter(DataLoader(replicated_ds, batch_size=batch_size))\n",
    "            # Warmup\n",
    "            for _ in range(2):\n",
    "                with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "                    _ = self.model(next(dataloader))\n",
    "\n",
    "            dataloader = iter(DataLoader(replicated_ds, batch_size=batch_size))\n",
    "            # Timed run\n",
    "            num_loops = num_iters * (max(batch_sizes) / batch_size)\n",
    "            assert int(num_loops) == num_loops\n",
    "            \n",
    "            for _ in range(int(num_loops)):\n",
    "                start_time = perf_counter()\n",
    "                with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "                    _ = self.model(next(dataloader))\n",
    "                latency =  perf_counter() - start_time\n",
    "                throughputs.append(batch_size / latency)\n",
    "                \n",
    "            # Compute run statistics\n",
    "            avg_throughput = np.mean(throughputs)\n",
    "            std_throughput = np.std(throughputs)\n",
    "            print(rf\"Average throughput (bs={batch_size}) (samples/second): {avg_throughput:.2f} +\\- {std_throughput:.2f}\\n\")\n",
    "            results[f\"throughput_avg_{batch_size}\"] = avg_throughput\n",
    "            results[f\"throughput_std_{batch_size}\"] = std_throughput\n",
    "        return results\n",
    "        \n",
    "    def run_benchmark(self):\n",
    "        metrics = {self.optim_type: {}}\n",
    "        metrics[self.optim_type].update(self.compute_size())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        metrics[self.optim_type].update(self.compute_latency())\n",
    "        metrics[self.optim_type].update(self.compute_throughput())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "PurksLh3qcBa",
   "metadata": {
    "id": "PurksLh3qcBa"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_metrics(perf_metrics):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        plt.errorbar(\n",
    "            df_opt[\"time_avg_ms\"],\n",
    "            df_opt[\"accuracy\"] * 100,\n",
    "            xerr=df_opt[\"time_std_ms\"],\n",
    "            fmt=\"o\",\n",
    "            alpha=0.5,\n",
    "            ms=df_opt[\"size_mb\"] / 15,\n",
    "            label=idx,\n",
    "            capsize=5,\n",
    "            capthick=1,\n",
    "        )\n",
    "\n",
    "    legend = plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.ylim(63, 95)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = max([metrics[\"time_avg_ms\"] for metrics in perf_metrics.values()]) * 1.3\n",
    "    plt.xlim(0, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency with batch_size=1 (ms)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-w99Y2qW4lU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-w99Y2qW4lU",
    "outputId": "57f0b8f7-6dad-4e90-c779-658a7de6e960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 127.32\n",
      "Accuracy on test set - 0.906\n",
      "Average latency (ms) - 8.74 +\\- 0.28\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "small_model = SetFitModel.from_pretrained(\"moshew/bge-small-en-v1.5_setfit-sst2-english\")\n",
    "pb = PerformanceBenchmark(model=small_model, dataset=test_dataset, optim_type=\"bge-small (PyTorch)\")\n",
    "perf_metrics = pb.run_benchmark()\n",
    "with open(\"perf_metrics.json\", \"w\") as f:\n",
    "    json.dump(perf_metrics, f)\n",
    "plot_metrics(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AiPUhOCNWRny",
   "metadata": {
    "id": "AiPUhOCNWRny"
   },
   "source": [
    "## 4. Optimizing with [`optimum-intel`](https://github.com/huggingface/optimum-intel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ea1d8-b82a-4a88-b3eb-2fa87582e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install intel-optimum and dependencies\n",
    "# !python -m pip install torch==2.1 --index-url https://download.pytorch.org/whl/cpu -qqq\n",
    "# !python -m pip install intel-extension-for-pytorch -qqq\n",
    "# !python -m pip install intel-openmp -qqq\n",
    "# !python -m pip install optimum[neural-compressor] --upgrade-strategy eager -qqq\n",
    "# !python -m pip install mteb simple-parsing -qqq\n",
    "\n",
    "# !cd ~ && git clone https://github.com/IntelLabs/fastRAG.git && cd -\n",
    "\n",
    "# RESTART KERNEL AND RUN CELL [2] AGAIN\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acba1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize our SetFit model using 100 samples:\n",
    "\n",
    "QUANIIZTER=\"~/fastRAG/scripts/optimizations/embedders/quantize_embedder.py\"\n",
    "!python $QUANIIZTER --quantize --model_name \"moshew/bge-small-en-v1.5_setfit-sst2-english\" --output_path bge-small-en-v1.5_setfit-sst2-quant/ \\\n",
    "    --sample_size 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enaQpBF9WRn9",
   "metadata": {
    "id": "enaQpBF9WRn9"
   },
   "outputs": [],
   "source": [
    "from setfit.exporters.utils import mean_pooling\n",
    "\n",
    "class INCSetFitModel:\n",
    "    def __init__(self, inc_model, tokenizer, model_head):\n",
    "        self.inc_model = inc_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_head = model_head\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(self.inc_model.device)\n",
    "\n",
    "        outputs = self.inc_model(**encoded_inputs)\n",
    "        embeddings = mean_pooling(\n",
    "            outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"]\n",
    "        )\n",
    "        return self.model_head.predict(embeddings.cpu())\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qRviEk2WWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRviEk2WWRn9",
    "outputId": "33f010a8-376e-4f0c-b21b-97fe25bf1a81"
   },
   "outputs": [],
   "source": [
    "from optimum.intel import INCModel\n",
    "from transformers import AutoTokenizer\n",
    "from setfit import SetFitModel\n",
    "from datasets import load_dataset\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"]\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "inc_model_path = \"bge-small-en-v1.5_setfit-sst2-quant\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(inc_model_path, model_max_length=512)\n",
    "inc_model = INCModel.from_pretrained(inc_model_path)\n",
    "model = SetFitModel.from_pretrained(\"moshew/bge-small-en-v1.5_setfit-sst2-english\")\n",
    "inc_setfit_model = INCSetFitModel(inc_model, tokenizer, model.model_head)\n",
    "\n",
    "# Perform inference\n",
    "inc_setfit_model(test_dataset[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8jpZ3gdWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8jpZ3gdWRn9",
    "outputId": "8d31c81a-67e4-4074-cf35-9f56d6dcdd20"
   },
   "outputs": [],
   "source": [
    "class INCPerformanceBenchmark(PerformanceBenchmark):\n",
    "    def __init__(self, *args, model_path, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_path = model_path\n",
    "\n",
    "    def compute_size(self):\n",
    "        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n",
    "        print(f\"Model size (MB) - {size_mb:.3f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "        \n",
    "pb = INCPerformanceBenchmark(\n",
    "    inc_setfit_model,\n",
    "    test_dataset,\n",
    "    \"bge-small (optimum-intel)\",\n",
    "    model_path=inc_model_path,\n",
    ")\n",
    "\n",
    "with open(\"perf_metrics.json\") as f:\n",
    "    perf_metrics = json.load(f)\n",
    "perf_metrics.update(pb.run_benchmark())\n",
    "\n",
    "for optim_type, results in perf_metrics.items():\n",
    "    s = [f\"{k}: {v:.3f}\" for k, v in results.items()]\n",
    "    print(f\"{optim_type}:\\n\" + \"\\n\".join(s) + \"\\n\")\n",
    "    \n",
    "plot_metrics(perf_metrics)\n",
    "\n",
    "def get_latency_speedup(model_name):\n",
    "    speedup = perf_metrics['bge-small (PyTorch)']['time_avg_ms'] / perf_metrics[model_name]['time_avg_ms']\n",
    "    return speedup\n",
    "print(f\"Latency speedup for 'bge-small (optimum-intel)': {get_latency_speedup('bge-small (optimum-intel)'):.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1adb0c-cee2-4f02-ae0d-04bd1e63d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
