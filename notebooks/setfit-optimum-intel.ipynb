{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76571396-8f54-40ed-9e81-6c7531e6eaee",
   "metadata": {
    "id": "76571396-8f54-40ed-9e81-6c7531e6eaee"
   },
   "source": [
    "# Accelerate SetFit Models with ðŸ¤— Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733f283-4736-4d9d-854e-5582280b074d",
   "metadata": {},
   "source": [
    "In this notebook, we'll show how you can use ðŸ¤— [Optimum Intel](https://github.com/huggingface/optimum-intel) to speed up your SetFit models on Intel CPUs.\\\n",
    "Optimum Intel is the interface between the ðŸ¤— Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n",
    "\n",
    "In our experiments, we will be using a fine-tuned SetFit model based on `bge-small-en-v1.5` sentence transformer model and observe our performance increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874cea-d8d1-4fdf-b374-7772811fe7b7",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbde933-a2f1-4159-9b93-cfcf73f28c79",
   "metadata": {},
   "source": [
    "* Linux OS\n",
    "* Python 3.9+\n",
    "* An Intel Xeon Processor which supports [bfloat16](https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-on-xeon-processors-with-bfloat16.html)\n",
    "\n",
    "Since Google Colab doesn't meet the hardware requirement, it's incompatible with this notebook and will not see speedups from our optimizations.\n",
    "\n",
    "Please run the following to ensure you meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9247a-b053-46af-89f6-e57067ecb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "os_info = !uname -o\n",
    "os_type = os_info[0]\n",
    "is_os_ok = \"linux\" in os_type.lower()\n",
    "is_python_ver_ok = sys.version_info.major == 3 and sys.version_info.minor >=9\n",
    "cpu_info = !lscpu\n",
    "is_cpu_ok = \"intel\" in cpu_info.grep(\"vendor\")[0].lower() and \"bf16\" in cpu_info.grep(\"flags\")[0]\n",
    "if not is_os_ok:\n",
    "    print(f\"This notebook requires a Linux type operating system, but you have {os_type}\")\n",
    "if not is_python_ver_ok:\n",
    "    print(f\"This notebook requires Python 3.9+ but your version is {sys.version.split()[0]}.\")\n",
    "if not is_cpu_ok:\n",
    "    print(f\"Your CPU does not support bfloat16.\")\n",
    "print(f\"Notebook requirements {'' if is_os_ok and is_python_ver_ok and is_cpu_ok else 'not'} met.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855b561-fea5-4137-ab78-e3c32a200748",
   "metadata": {},
   "source": [
    "### Before Running this Notebook\n",
    "1. Please create a `conda` environment and install `jupyterlab`:\n",
    "```bash\n",
    "conda create -n setfit python=3.10 -y && conda activate setfit\n",
    "python -m pip install -U jupyterlab\n",
    "```\n",
    "\n",
    "2. Install **TCMalloc**: Google's fast, multi-threaded `malloc()` implementation:\n",
    "```bash\n",
    "conda install gperftools -c conda-forge -y\n",
    "echo export LD_PRELOAD=${CONDA_PREFIX}/lib/libtcmalloc.so:$LD_PRELOAD >> ~/.bashrc\n",
    "# **** Restart the shell or `source ~/.bashrc` ****\n",
    "```\n",
    "\n",
    "3. Launch the notebook using `numactl` (explained below), for example: \n",
    "```bash\n",
    "OMP_NUM_THREADS=56 numactl -C 0-55 -m 0 jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c08bea-1c62-450b-b532-068bbd6bc92d",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "This notebook was tested on `Ubuntu 22.04.3 LTS` and `Python 3.10.13`.\n",
    "\n",
    "To reproduce the maximum speeds reported in this notebook, please launch it using `numactl`.\\\n",
    "`numactl` enables gaining better control on which resources you're running. It's best to run on a single socket (`-m 0`):\\\n",
    "`OMP_NUM_THREADS=<NUM_THREADS> numactl -C <CORES_RANGE> -m 0 jupyter lab`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc2a85",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Install the required packages to evaluate and visualize results for SetFit models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U setfit evaluate matplotlib -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72d255-ab98-4cc3-878b-6a60e08ccf6d",
   "metadata": {},
   "source": [
    "### Install Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cefb9e-67b8-40d2-9448-5cb54527555f",
   "metadata": {},
   "source": [
    "Install Optimum Intel along with INC and IPEX backends which we'll use in our optimization later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9466ee2-5791-45b4-b69d-33bb5a82f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade-strategy eager optimum[neural-compressor] -qqq\n",
    "!python -m pip install intel-extension-for-pytorch -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493cd04-812f-461b-bb26-9225d76c2774",
   "metadata": {},
   "source": [
    "### Set Up Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a090c0-a065-4b3f-b803-5e518c73299f",
   "metadata": {},
   "source": [
    "First, define the infrastructure for conducting latency, throughput and accuracy benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55756fec-fc22-4590-84d7-2f3df37b9256",
   "metadata": {
    "id": "55756fec-fc22-4590-84d7-2f3df37b9256"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.exporters.utils import mean_pooling\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_dir_size(path):\n",
    "    ps = subprocess.Popen(('du','-hk', path), stdout=subprocess.PIPE)\n",
    "    output = subprocess.check_output((\"awk\", \"{print $1/1024,$2}\"), stdin=ps.stdout)\n",
    "    ps.wait()\n",
    "    return float(output.split()[0].decode('utf-8'))\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, model, dataset, optim_type, metric=\"accuracy\", max_sequence_length=256,\n",
    "                 model_path=None, enable_autocast=False, batch_sizes=None):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        self.metric = evaluate.load(metric)\n",
    "        self.model_path = model_path\n",
    "        self.enable_autocast = enable_autocast\n",
    "        self.tokenizer = self.model.model_body.tokenizer\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.max_length = max_sequence_length\n",
    "        self.device = self.model.model_body.device\n",
    "        self.batch_sizes = [2 ** i for i in range(12)] if batch_sizes is None else batch_sizes\n",
    "\n",
    "    def to_device(self, batch):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(self.device)\n",
    "        return batch\n",
    "        \n",
    "    def timed_predict(self, inputs=None, encoded_inputs=None):\n",
    "        if encoded_inputs is None:\n",
    "            encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        start = perf_counter()\n",
    "        encoded_inputs = self.to_device(encoded_inputs)\n",
    "        with torch.no_grad(), torch.cpu.amp.autocast(dtype=torch.bfloat16, enabled=self.enable_autocast):\n",
    "            if isinstance(self.model.model_body, SentenceTransformer):\n",
    "                embeddings = self.model.model_body(encoded_inputs)[\"sentence_embedding\"]\n",
    "            else:\n",
    "                outputs = self.model.model_body(**encoded_inputs)\n",
    "                embeddings = mean_pooling(outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"])\n",
    "            preds = self.model.model_head.predict(embeddings)\n",
    "            latency = perf_counter() - start\n",
    "        return preds, latency\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        preds, _ = self.timed_predict(self.dataset[\"text\"])\n",
    "        accuracy = self.metric.compute(predictions=preds, references=self.dataset[\"label\"])\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "        \n",
    "    def compute_size(self):\n",
    "        if self.model_path is None:\n",
    "            state_dict = self.model.model_body.state_dict()\n",
    "            tmp_path = Path(\"model.pt\")\n",
    "            torch.save(state_dict, tmp_path)\n",
    "            size_mb = get_dir_size(tmp_path)\n",
    "            # Delete temporary file\n",
    "            tmp_path.unlink()\n",
    "        else:\n",
    "            size_mb = get_dir_size(self.model_path)\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def generate_random_sequences(self, batch_size):\n",
    "        input_ids = torch.randint(0, self.vocab_size - 1, (batch_size, self.max_length))\n",
    "        token_type_ids = torch.zeros((batch_size, self.max_length), dtype=torch.int64)\n",
    "        attention_mask = torch.ones((batch_size, self.max_length), dtype=torch.int64)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def compute_latency_and_throughput(self, num_iter=15, num_warmup_iter=5):\n",
    "        res = defaultdict(list)\n",
    "        print(\"=\" * 100)\n",
    "        for batch_size in self.batch_sizes:\n",
    "            try:\n",
    "                for _ in trange(num_warmup_iter, desc=f\"Warmup ({batch_size=})\", file=sys.stdout):\n",
    "                    encoded_inputs = self.generate_random_sequences(batch_size=batch_size)\n",
    "                    _ = self.timed_predict(encoded_inputs=encoded_inputs)\n",
    "\n",
    "                batch_latencies = []\n",
    "                for _ in trange(num_iter, desc=f\"Timed Run ({batch_size=})\", file=sys.stdout):\n",
    "                    encoded_inputs = self.generate_random_sequences(batch_size=batch_size)\n",
    "                    _, latency = self.timed_predict(encoded_inputs=encoded_inputs)\n",
    "                    batch_latencies.append(latency)\n",
    "        \n",
    "                # Compute run statistics\n",
    "                batch_latencies = np.array(batch_latencies)\n",
    "                if batch_size == 1:\n",
    "                    latency = 1000 * (batch_latencies / batch_size)\n",
    "                    res.update({\"time_avg_ms\": np.mean(latency), \"time_std_ms\": np.std(latency)})\n",
    "                    print(f\"Avg. latency (ms), {batch_size=} - {res['time_avg_ms']:.2f} +/- {res['time_std_ms']:.2f}\")\n",
    "\n",
    "                throughputs = batch_size / batch_latencies\n",
    "                res[\"throughputs_avg\"].append(np.mean(throughputs))\n",
    "                res[\"throughputs_std\"].append(np.std(throughputs))\n",
    "                print(f\"Avg. throughput (samples/sec), {batch_size=} - {res['throughputs_avg'][-1]:.1f} +/- {res['throughputs_std'][-1]:.1f}\")\n",
    "                res[\"batch_sizes\"].append(batch_size)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping throughput computation at {batch_size=} since it failed with the following error: \\n\\n{e}\\n\")\n",
    "                print(e.stack())\n",
    "        return res\n",
    "        \n",
    "    def run_benchmark(self):\n",
    "        all_metrics = {}\n",
    "        for run_metric in  self.compute_latency_and_throughput, self.compute_size, self.compute_accuracy:\n",
    "            all_metrics |= run_metric()\n",
    "        return {self.optim_type: all_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878768f-9ec4-4675-ab20-0f8bca7bbe37",
   "metadata": {},
   "source": [
    "Let's add a function to plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99110d37-1aa1-4373-8517-f1f8acbbc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(perf_metrics):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "    plt.figure(figsize=(6, 4), dpi=120)\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        plt.errorbar(\n",
    "            df_opt[\"time_avg_ms\"],\n",
    "            df_opt[\"accuracy\"] * 100,\n",
    "            xerr=df_opt[\"time_std_ms\"],\n",
    "            fmt=\"o\",\n",
    "            alpha=0.5,\n",
    "            ms=df_opt[\"size_mb\"] / 15,\n",
    "            label=idx,\n",
    "            capsize=5,\n",
    "            capthick=1,\n",
    "        )\n",
    "    legend = plt.legend(loc=\"lower right\")\n",
    "    plt.ylim(63, 95)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = max([metrics[\"time_avg_ms\"] for metrics in perf_metrics.values()]) * 1.3\n",
    "    plt.xlim(0, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency with batch_size=1 (ms)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159ea96-f604-478c-b6a2-bf4cc5c57a5a",
   "metadata": {},
   "source": [
    "Load the dataset for running evaluations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842dea2-76cc-408b-9431-0b282b9abaed",
   "metadata": {},
   "source": [
    "## 1. Benchmark SetFit using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a9450-d8f0-4697-b3b4-a5ddf6b04906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b47c9-7076-4510-8c1e-5c17445bae34",
   "metadata": {},
   "source": [
    "We'll now run the benchmark with the standard PyTorch backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-w99Y2qW4lU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-w99Y2qW4lU",
    "outputId": "57f0b8f7-6dad-4e90-c779-658a7de6e960"
   },
   "outputs": [],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "model_path = \"dkorat/bge-small-en-v1.5_setfit-sst2-english\"\n",
    "small_model = SetFitModel.from_pretrained(model_path)\n",
    "pb = PerformanceBenchmark(small_model, test_dataset, \"bge-small (PyTorch)\")\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AiPUhOCNWRny",
   "metadata": {
    "id": "AiPUhOCNWRny"
   },
   "source": [
    "## 2. Optimize with Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60d191-db3b-49a3-a281-34ae9063cb25",
   "metadata": {},
   "source": [
    "In order to optimize our SetFit model, we will apply quantization to the model body, using Intel [Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC), part of Optimum Intel.\n",
    "\n",
    "**Quantization** is a very popular deep learning model optimization technique for improving inference speeds. It minimizes the number of bits required to represent the weights and/or activations in a neural network. This is done by converting a set of real-valued numbers into their lower-bit data representations, such as INT8. Moreover, quantization can enable faster computations in lower precision.\n",
    "\n",
    "Specifically, we'll apply post-training static quantization (PTQ). PTQ can reduce the memory footprint and latency for inference, while still preserving the accuracy of the model, with only a small unlabeled calibration set and without any training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0557f9-8193-4059-8056-bfc50c00deef",
   "metadata": {},
   "source": [
    "#### Prepare a Calibration Dataset\n",
    "\n",
    "The calibration dataset should be able to represent the data distribution of unseen data. In general, preparing 100 samples is enough for calibration. We'll use the Qasper dataset in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8466be-f28d-4303-8511-15798806a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_qasper_calibration_set(sample_size) -> Dataset:\n",
    "    train_set = load_dataset(\"allenai/qasper\")[\"train\"]\n",
    "    random.seed(666)\n",
    "    random_samples = random.sample(range(len(train_set)), sample_size)\n",
    "    random_queries = [random.sample(train_set[x][\"qas\"][\"question\"], 1)[0] for x in random_samples]\n",
    "    random_abstracts = [train_set[x][\"abstract\"] for x in random_samples]\n",
    "    samples = random.sample(random_queries + random_abstracts, sample_size)\n",
    "    random.shuffle(samples)\n",
    "    def gen():\n",
    "        for s in samples:\n",
    "            yield {\"text\": s}\n",
    "    return Dataset.from_generator(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90c6ba-8f6e-4899-b404-ecebc0f7a78a",
   "metadata": {},
   "source": [
    "#### Run Quantization\n",
    "Define the desired quantization process using `optimum.intel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7739dc-2a56-4960-8f56-7f3f74991fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optimum.intel\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "def quantize(setfit_model, output_path, calibration_set):\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True)\n",
    "        \n",
    "    model = setfit_model.model_body[0].auto_model\n",
    "    tokenizer = setfit_model.model_body.tokenizer\n",
    "    vectorized_ds = calibration_set.map(preprocess_function, num_proc=10)\n",
    "    vectorized_ds = vectorized_ds.remove_columns([\"text\"])\n",
    "    quantizer = optimum.intel.INCQuantizer.from_pretrained(model)\n",
    "    quantization_config = PostTrainingQuantConfig(approach=\"static\", backend=\"ipex\", domain=\"nlp\")\n",
    "    quantizer.quantize(\n",
    "        quantization_config=quantization_config,\n",
    "        calibration_dataset=vectorized_ds,\n",
    "        save_directory=output_path,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    tokenizer.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f935e3-d09a-4f44-87a0-f773b83e569a",
   "metadata": {},
   "source": [
    "Quantize our SetFit model on 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508125d2-7bc4-4a6a-9b9d-1243ac0ff002",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"dkorat/bge-small-en-v1.5_setfit-sst2-english\"\n",
    "model = SetFitModel.from_pretrained(model_path)\n",
    "calibration_set = load_qasper_calibration_set(sample_size=100)\n",
    "optimum_model_path = \"/tmp/bge-small-en-v1.5_setfit-sst2-english_opt\"\n",
    "quantize(model, output_path=optimum_model_path, calibration_set=calibration_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8d9eb-354c-4d0f-8b68-d1cb3417dd29",
   "metadata": {},
   "source": [
    "#### Benchmark Optimum Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424628ae-091b-47bd-952f-96e1d6e7a1b2",
   "metadata": {},
   "source": [
    "This next class defines a wrapper around our SetFit model which plugs in our quantized model at inference (instead of the original model body)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e47ade-06ba-4a5f-8f7a-d35ccf8d8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimum.intel\n",
    "\n",
    "class OptimumSetFitModel:\n",
    "    def __init__(self, setfit_model, model_body):\n",
    "        model_body.tokenizer = setfit_model.model_body.tokenizer\n",
    "        self.model_body = model_body\n",
    "        self.model_head = setfit_model.model_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7215f",
   "metadata": {},
   "source": [
    "Time to run the performance benchmark on our optimized SetFit model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941e895-72e2-43e6-a3f4-9af39a5f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_model = optimum.intel.IPEXModel.from_pretrained(optimum_model_path)\n",
    "optimum_setfit_model = OptimumSetFitModel(model, model_body=optimum_model)\n",
    "\n",
    "pb = PerformanceBenchmark(\n",
    "    optimum_setfit_model,\n",
    "    test_dataset,\n",
    "    f\"bge-small (optimum intel)\",\n",
    "    model_path=optimum_model_path,\n",
    "    enable_autocast=True,\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b2998-6605-4e42-9fb5-247c5eee750f",
   "metadata": {},
   "source": [
    "#### Plot Latencies and Compute Latency Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8jpZ3gdWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8jpZ3gdWRn9",
    "outputId": "8d31c81a-67e4-4074-cf35-9f56d6dcdd20"
   },
   "outputs": [],
   "source": [
    "plot_metrics(perf_metrics)\n",
    "speedup = perf_metrics['bge-small (PyTorch)']['time_avg_ms'] / perf_metrics['bge-small (optimum intel)']['time_avg_ms']\n",
    "print(f\"Latency speedup for 'bge-small (optimum-intel)': {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48568c7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### After optimizing, inference is 3.7x faster than before, with no drop in accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6872f-1765-48a5-beba-a989555b5461",
   "metadata": {},
   "source": [
    "#### Throughput Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e043497-89c0-44a7-b08f-1f8012cd9fba",
   "metadata": {},
   "source": [
    "Let's look now at the throughput, which is the number of samples the model can predict per second.\\\n",
    "We'll plot this value for our optimized and baseline models, as a function of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701770a-d643-4f32-bd56-2574ba7dc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_throughputs(perf_metrics):\n",
    "    plt.figure(figsize=(10, 6), dpi=120)\n",
    "    for (label, metric) in perf_metrics.items():\n",
    "        xs = np.arange(len(metric[\"batch_sizes\"]))\n",
    "        y_mean, y_std = np.array(metric[\"throughputs_avg\"]), np.array(metric[\"throughputs_std\"])\n",
    "        plt.plot(xs, y_mean, label=label, alpha=0.8)\n",
    "        plt.fill_between(xs, y_mean - y_std, y_mean + y_std, alpha=0.2)\n",
    "        for x, y in zip(xs, y_mean):\n",
    "            plt.text(x, y, f'{y:.1f}', ha='right', va='bottom', fontsize=7, color=\"black\")\n",
    "        \n",
    "    plt.xticks(xs, pb.batch_sizes[:len(xs)])\n",
    "    plt.ylabel('Samples/Second')\n",
    "    plt.suptitle('Throughput vs Batch Size')\n",
    "    plt.title('\\nBAAI/bge-small-en-v1.5 SetFit model trained on SetFit/sst2', fontsize=8)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_throughputs(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6ab3-0d71-453a-9af3-67f8947fa6ce",
   "metadata": {},
   "source": [
    "#### Similarly to the latency speedup, we can see that our optimization has resulted in up to 3.5x throughput increase as well!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dk-setfit-310",
   "language": "python",
   "name": "dk-setfit-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
