{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76571396-8f54-40ed-9e81-6c7531e6eaee",
   "metadata": {
    "id": "76571396-8f54-40ed-9e81-6c7531e6eaee"
   },
   "source": [
    "# Accelerate SetFit Models with 🤗 Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733f283-4736-4d9d-854e-5582280b074d",
   "metadata": {},
   "source": [
    "In this notebook, we'll show how you can use 🤗 [Optimum Intel](https://github.com/huggingface/optimum-intel) to speed up your SetFit models on Intel CPUs.\\\n",
    "[Optimum Intel](https://github.com/huggingface/optimum-intel) is an open-source library that accelerates end-to-end pipelines built with Hugging Face libraries on Intel Hardware. Optimum Intel includes several techniques to accelerate models such as low-bit quantization, model weight pruning, distillation, and an accelerated runtime.\n",
    "\n",
    "Optimizing pre-trained models can be done easily with Optimum Intel; many simple examples can be found [here](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc).\n",
    "\n",
    "In our experiments, we will be using a fine-tuned SetFit model based on [`bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5) [Sentence Transformer](https://www.sbert.net/) model and observe our performance increase. \\\n",
    "We'll compare the performance of the Optimum Intel optimization with two other commonly used methods:\n",
    "\n",
    " - Using PyTorch and Huggingface's Transformers library with fp32.\n",
    " - Using [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) runtime with bf16 and tracing the model using torchscript.\n",
    "\n",
    "For further details, please refer to our [blog]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874cea-d8d1-4fdf-b374-7772811fe7b7",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbde933-a2f1-4159-9b93-cfcf73f28c79",
   "metadata": {},
   "source": [
    "* Linux OS\n",
    "* Python 3.9+\n",
    "* An Intel Xeon Processor which supports [bfloat16](https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-on-xeon-processors-with-bfloat16.html)\n",
    "\n",
    "Since Google Colab doesn't meet the hardware requirement, it's incompatible with this notebook and will not see speedups from our optimizations.\n",
    "\n",
    "Please run the following to ensure you meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d9247a-b053-46af-89f6-e57067ecb8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook requirements  met.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "os_info = !uname -o\n",
    "os_type = os_info[0]\n",
    "is_os_ok = \"linux\" in os_type.lower()\n",
    "is_python_ver_ok = sys.version_info.major == 3 and sys.version_info.minor >=9\n",
    "cpu_info = !lscpu\n",
    "is_cpu_ok = \"intel\" in cpu_info.grep(\"vendor\")[0].lower() and \"bf16\" in cpu_info.grep(\"flags\")[0]\n",
    "if not is_os_ok:\n",
    "    print(f\"This notebook requires a Linux type operating system, but you have {os_type}\")\n",
    "if not is_python_ver_ok:\n",
    "    print(f\"This notebook requires Python 3.9+ but your version is {sys.version.split()[0]}.\")\n",
    "if not is_cpu_ok:\n",
    "    print(f\"Your CPU does not support bfloat16.\")\n",
    "print(f\"Notebook requirements {'' if is_os_ok and is_python_ver_ok and is_cpu_ok else 'not'} met.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855b561-fea5-4137-ab78-e3c32a200748",
   "metadata": {},
   "source": [
    "### Before Running this Notebook\n",
    "1. Please create a `conda` environment and install `jupyterlab`:\n",
    "```bash\n",
    "conda create -n setfit python=3.10 -y && conda activate setfit\n",
    "python -m pip install -U jupyterlab\n",
    "```\n",
    "\n",
    "2. Install **TCMalloc**: Google's fast, multi-threaded `malloc()` implementation:\n",
    "```bash\n",
    "conda install gperftools -c conda-forge -y\n",
    "echo export LD_PRELOAD=${CONDA_PREFIX}/lib/libtcmalloc.so:$LD_PRELOAD >> ~/.bashrc\n",
    "# **** Restart the shell or `source ~/.bashrc` ****\n",
    "```\n",
    "\n",
    "3. To reproduce the maximum speeds reported in this notebook, please launch it using `numactl`.\\\n",
    "`numactl` enables gaining better control on which resources you're running. \\\n",
    "It's best to run on a single socket (`-m 0`):\n",
    "```bash\n",
    "OMP_NUM_THREADS=<NUM_THREADS> numactl -C <CORES_RANGE> -m 0 jupyter lab\n",
    "```\n",
    "\n",
    "**Note**: For even increased throughput, the data can be split into 2 processes, each running seperately on a different socket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c08bea-1c62-450b-b532-068bbd6bc92d",
   "metadata": {},
   "source": [
    "#### Experimental Setup\n",
    "- Hardware (CPU): 5th gen Intel Xeon 8580 with 2 sockets, 60 cores per socket.\n",
    "- This notebook was tested on `Ubuntu 22.04.3 LTS` and `Python 3.10.13`.\n",
    "- All runs were evaluated with 60 cores on 1 CPU socket, and cores ranging from 0-59.\n",
    "- TCMalloc was installed and defined as an environment variable in all runs. (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc2a85",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install the required packages to evaluate and visualize results for SetFit models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839bd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U setfit evaluate matplotlib -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72d255-ab98-4cc3-878b-6a60e08ccf6d",
   "metadata": {},
   "source": [
    "### Install Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cefb9e-67b8-40d2-9448-5cb54527555f",
   "metadata": {},
   "source": [
    "Install Optimum Intel along with INC and IPEX backends which we'll use in our optimization later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9466ee2-5791-45b4-b69d-33bb5a82f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade-strategy eager optimum[neural-compressor] -qqq\n",
    "!python -m pip install intel-extension-for-pytorch -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493cd04-812f-461b-bb26-9225d76c2774",
   "metadata": {},
   "source": [
    "### Set Up Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a090c0-a065-4b3f-b803-5e518c73299f",
   "metadata": {},
   "source": [
    "First, define the infrastructure for conducting latency, throughput and accuracy benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55756fec-fc22-4590-84d7-2f3df37b9256",
   "metadata": {
    "id": "55756fec-fc22-4590-84d7-2f3df37b9256"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import trange\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from setfit.exporters.utils import mean_pooling\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_dir_size(path):\n",
    "    ps = subprocess.Popen(('du','-hk', path), stdout=subprocess.PIPE)\n",
    "    output = subprocess.check_output((\"awk\", \"{print $1/1024,$2}\"), stdin=ps.stdout)\n",
    "    ps.wait()\n",
    "    return float(output.split()[0].decode('utf-8'))\n",
    "\n",
    "def generate_random_sequences(batch_size, length, vocab_size):\n",
    "    input_ids = torch.randint(0, vocab_size - 1, (batch_size, length))\n",
    "    token_type_ids = torch.zeros((batch_size, length), dtype=torch.int64)\n",
    "    attention_mask = torch.ones((batch_size, length), dtype=torch.int64)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "    \n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, model, dataset, optim_type, max_sequence_length=512, metric=\"accuracy\",\n",
    "                 model_path=None, autocast_dtype=None, batch_sizes=None):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        self.metric = evaluate.load(metric)\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = self.model.model_body.tokenizer\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.max_length = max_sequence_length\n",
    "        self.device = self.model.model_body.device\n",
    "        self.batch_sizes = [2 ** i for i in range(10)] if batch_sizes is None else batch_sizes\n",
    "        self.autocast_dtype = autocast_dtype\n",
    "        self.model.model_body.eval()\n",
    "            \n",
    "    def to_device(self, batch):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(self.device)\n",
    "        return batch\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def timed_predict(self, inputs=None, encoded_inputs=None):\n",
    "        if encoded_inputs is None:\n",
    "            encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=\"max_length\", max_length=self.max_length, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        start = perf_counter()\n",
    "        encoded_inputs = self.to_device(encoded_inputs)\n",
    "        \n",
    "        with torch.no_grad(), torch.cpu.amp.autocast(dtype=self.autocast_dtype, enabled=self.autocast_dtype is not None):                \n",
    "            if isinstance(self.model.model_body, SentenceTransformer) or \"ipex\" in self.optim_type:\n",
    "                embeddings = self.model.model_body(encoded_inputs)[\"sentence_embedding\"]\n",
    "            else:\n",
    "                # Quantized SetFit model bodies don't include mean pooling\n",
    "                outputs = self.model.model_body(**encoded_inputs)\n",
    "                embeddings = mean_pooling(outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"])\n",
    "                \n",
    "            preds = self.model.model_head.predict(embeddings)\n",
    "            latency = perf_counter() - start\n",
    "        return preds, latency\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        preds, _ = self.timed_predict(self.dataset[\"text\"])\n",
    "        accuracy = self.metric.compute(predictions=preds, references=self.dataset[\"label\"])\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "        \n",
    "    def compute_size(self):\n",
    "        if self.model_path is None:\n",
    "            tmp_path = Path(\"model.pt\")\n",
    "            try:\n",
    "                with open(tmp_path, \"wb\") as f:\n",
    "                    torch.jit.save(self.model.model_body, f)\n",
    "            except:\n",
    "                state_dict = self.model.model_body.state_dict()\n",
    "                torch.save(state_dict, tmp_path)\n",
    "            \n",
    "            size_mb = get_dir_size(tmp_path)\n",
    "            # Delete temporary file\n",
    "            tmp_path.unlink()\n",
    "        else:\n",
    "            size_mb = get_dir_size(self.model_path)\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def compute_latency_and_throughput(self, num_iter=100, num_warmup_iter=40):\n",
    "        res = defaultdict(list)\n",
    "        for batch_size in self.batch_sizes:\n",
    "            try:\n",
    "                for _ in trange(num_warmup_iter, desc=f\"Warmup ({batch_size=})\", file=sys.stdout):\n",
    "                    encoded_inputs = generate_random_sequences(batch_size=batch_size, length=self.max_length, vocab_size=self.vocab_size)\n",
    "                    _ = self.timed_predict(encoded_inputs=encoded_inputs)\n",
    "    \n",
    "                batch_latencies = []\n",
    "                for _ in trange(num_iter, desc=f\"Timed Run ({batch_size=})\", file=sys.stdout):\n",
    "                    encoded_inputs = generate_random_sequences(batch_size=batch_size, length=self.max_length, vocab_size=self.vocab_size)\n",
    "                    _, latency = self.timed_predict(encoded_inputs=encoded_inputs)\n",
    "                    batch_latencies.append(latency)\n",
    "        \n",
    "                # Compute run statistics\n",
    "                batch_latencies = np.array(batch_latencies)\n",
    "                if batch_size == 1:\n",
    "                    latency = 1000 * (batch_latencies / batch_size)\n",
    "                    res.update({\"time_avg_ms\": np.mean(latency), \"time_std_ms\": np.std(latency)})\n",
    "                    print(f\"Avg. latency (ms), {batch_size=} - {res['time_avg_ms']:.2f} +/- {res['time_std_ms']:.2f}\")\n",
    "    \n",
    "                throughputs = batch_size / batch_latencies\n",
    "                res[\"throughputs_avg\"].append(np.mean(throughputs))\n",
    "                res[\"throughputs_std\"].append(np.std(throughputs))\n",
    "                print(f\"Avg. throughput (samples/sec), {batch_size=} - {res['throughputs_avg'][-1]:.1f} +/- {res['throughputs_std'][-1]:.1f}\")\n",
    "                res[\"batch_sizes\"].append(batch_size)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping throughput computation at {batch_size=} since it failed with the following error: \\n\\n{e}\\n\")\n",
    "        return res\n",
    "        \n",
    "    def run_benchmark(self):\n",
    "        print(f\"\\n\\n{'*' * 150}\\nOptimization type: {self.optim_type}\\n{'*' * 150}\")\n",
    "        all_metrics = {}\n",
    "        for run_metric in self.compute_accuracy, self.compute_latency_and_throughput, self.compute_size:\n",
    "            all_metrics |= run_metric()\n",
    "            print(\"=\" * 100)\n",
    "        return {self.optim_type: all_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878768f-9ec4-4675-ab20-0f8bca7bbe37",
   "metadata": {},
   "source": [
    "Let's add a function to plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99110d37-1aa1-4373-8517-f1f8acbbc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(perf_metrics):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "    plt.figure(figsize=(6, 4), dpi=120)\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        x, y = df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100\n",
    "        plt.errorbar(\n",
    "            x,\n",
    "            y,\n",
    "            xerr=df_opt[\"time_std_ms\"],\n",
    "            fmt=\"o\",\n",
    "            alpha=0.5,\n",
    "            ms=df_opt[\"size_mb\"] / 15,\n",
    "            label=idx,\n",
    "            capsize=5,\n",
    "            capthick=1,\n",
    "        )\n",
    "        plt.text(x, y-3, f'{y:.1f}\\n{x:.1f}ms', va='bottom', fontsize=6, color=\"black\")\n",
    "        \n",
    "    plt.suptitle(\"Accuracy vs Latency (batch_size=1)\")\n",
    "    plt.title('\\nBAAI/bge-small-en-v1.5 SetFit model trained on SetFit/sst2', fontsize=8)\n",
    "    legend = plt.legend(loc=\"lower right\")\n",
    "    plt.ylim(63, 95)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = max([metrics[\"time_avg_ms\"] for metrics in perf_metrics.values()]) * 1.3\n",
    "    plt.xlim(0, xlim)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Latency (ms)\")\n",
    "    plt.savefig(\"accuracy_vs_latency.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842dea2-76cc-408b-9431-0b282b9abaed",
   "metadata": {},
   "source": [
    "## 1. Benchmark SetFit using 🤗 Transformers (Float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159ea96-f604-478c-b6a2-bf4cc5c57a5a",
   "metadata": {},
   "source": [
    "Load the dataset for running evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867a9450-d8f0-4697-b3b4-a5ddf6b04906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from setfit import SetFitModel\n",
    "\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"]\n",
    "\n",
    "# Prepare variables for benchmarks\n",
    "model_path = \"dkorat/bge-small-en-v1.5_setfit-sst2-english\"\n",
    "perf_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b47c9-7076-4510-8c1e-5c17445bae34",
   "metadata": {},
   "source": [
    "We'll now run the benchmark with the standard 🤗 Transformers backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u-w99Y2qW4lU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-w99Y2qW4lU",
    "outputId": "57f0b8f7-6dad-4e90-c779-658a7de6e960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "Optimization type: bge-small (transformers)\n",
      "******************************************************************************************************************************************************\n",
      "Accuracy on test set - 0.884\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cd79d1e01d4506a19621edc20e13f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=1):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc2ba1e4aad4aa581b37a6632666a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=1):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. latency (ms), batch_size=1 - 15.75 +/- 0.56\n",
      "Avg. throughput (samples/sec), batch_size=1 - 63.6 +/- 2.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb95aa302694b50aec8d3db8dd65921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=2):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cf30efd2c846209c9be9bce430bc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=2):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. throughput (samples/sec), batch_size=2 - 82.9 +/- 7.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab00d3096214313a2acce76c132a765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=4):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c648bf64c5d4ccbb975645a7d3d9570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=4):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. throughput (samples/sec), batch_size=4 - 109.5 +/- 1.7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ad51549603476db2b492d691862084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=8):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a724e13936f4463f8093f2b323c64424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=8):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. throughput (samples/sec), batch_size=8 - 119.1 +/- 6.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d344972aa81147b6aaa6821e52892146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=16):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dad0e278bc474f98f9568e51e49c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=16):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. throughput (samples/sec), batch_size=16 - 115.6 +/- 7.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaa0f7419744d27b094622b65d95bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=32):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8387b0f6a7b7493b85eda00866201426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=32):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. throughput (samples/sec), batch_size=32 - 83.8 +/- 3.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51191137425342b8b5e3c9e044cf1f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Warmup (batch_size=64):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a9491476e244c39878e560507ab2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Timed Run (batch_size=64):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setfit_model = SetFitModel.from_pretrained(model_path)\n",
    "pb = PerformanceBenchmark(\n",
    "    model=setfit_model, \n",
    "    dataset=test_dataset, \n",
    "    optim_type=\"bge-small (transformers)\",\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71722e2-7c40-43a3-ab85-de4398d81df7",
   "metadata": {},
   "source": [
    "## 2. Optimize with [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/#introduction) + [TorchScript](https://pytorch.org/docs/stable/jit.html) (BFloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52ad3a-12ab-4d25-8fc7-bf4703fb6aae",
   "metadata": {},
   "source": [
    "Let's now optimize by using Intel extension for PyTorch (IPEX) runtime with bf16 and tracing the model using torchscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016316c3-30a0-4dd1-9975-1bfb95613122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "setfit_model = SetFitModel.from_pretrained(model_path)\n",
    "model = setfit_model.model_body\n",
    "model.eval()\n",
    "dtype = torch.bfloat16\n",
    "optimized_model = ipex.optimize(model, dtype=dtype)\n",
    "d = generate_random_sequences(batch_size=1, length=model.tokenizer.model_max_length, vocab_size=model.tokenizer.vocab_size)\n",
    "\n",
    "optimized_model = torch.jit.trace(optimized_model, (d,), check_trace=False, strict=False)\n",
    "setfit_model.model_body = torch.jit.freeze(optimized_model)\n",
    "setfit_model.model_body.tokenizer = model.tokenizer\n",
    "setfit_model.model_body.device = model.device\n",
    "\n",
    "pb = PerformanceBenchmark(\n",
    "    model=setfit_model,\n",
    "    dataset=test_dataset,\n",
    "    optim_type=f\"bge-small (ipex-{str(dtype).split('.')[1]})\",\n",
    "    autocast_dtype=dtype,\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AiPUhOCNWRny",
   "metadata": {
    "id": "AiPUhOCNWRny"
   },
   "source": [
    "## 3. Optimize with 🤗 Optimum Intel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60d191-db3b-49a3-a281-34ae9063cb25",
   "metadata": {},
   "source": [
    "In order to optimize our SetFit model, we will apply quantization to the model body, using Intel [Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC), part of Optimum Intel.\n",
    "\n",
    "**Quantization** is a very popular deep learning model optimization technique for improving inference speeds. It minimizes the number of bits required to represent the weights and/or activations in a neural network. This is done by converting a set of real-valued numbers into their lower-bit data representations, such as INT8. Moreover, quantization can enable faster computations in lower precision.\n",
    "\n",
    "Specifically, we'll apply post-training static quantization (PTQ). PTQ can reduce the memory footprint and latency for inference, while still preserving the accuracy of the model, with only a small unlabeled calibration set and without any training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0557f9-8193-4059-8056-bfc50c00deef",
   "metadata": {},
   "source": [
    "#### Prepare a Calibration Dataset\n",
    "\n",
    "The calibration dataset should be able to represent the data distribution of unseen data. In general, preparing 100 samples is enough for calibration. We'll use the `rotten_tomatoes` dataset in our case, since it’s composed of movie reviews, like our target dataset, sst2. First, we’ll load 100 random samples from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2ae65-fc24-4ce2-9125-2100e78ed18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_set = load_dataset(\"rotten_tomatoes\")[\"train\"].shuffle(seed=42).select(range(100))  \n",
    "calibration_set[\"text\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471a9fb-52f0-4b79-9126-73d9de2b280f",
   "metadata": {},
   "source": [
    "To prepare the dataset for quantization, we'll need to tokenize each example. We won’t need the “text” and “label” columns, so let’s remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06480d69-f706-4eb6-90dd-e9bb0001625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True)\n",
    "\n",
    "setfit_model = SetFitModel.from_pretrained(model_path)\n",
    "tokenizer = setfit_model.model_body.tokenizer\n",
    "calibration_set = calibration_set.map(tokenize, num_proc=10).remove_columns([\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90c6ba-8f6e-4899-b404-ecebc0f7a78a",
   "metadata": {},
   "source": [
    "#### Run Quantization\n",
    "Now we define the desired quantization process - in our case - Static Post Training Quantization, and use `optimum.intel` to run the quantization on our calibration dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c5008-77a0-4985-bb99-7108d57af5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import INCQuantizer\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "\n",
    "setfit_body = setfit_model.model_body[0].auto_model\n",
    "quantizer = INCQuantizer.from_pretrained(setfit_body)\n",
    "optimum_model_path = \"/tmp/bge-small-en-v1.5_setfit-sst2-english_opt\"\n",
    "quantization_config = PostTrainingQuantConfig(approach=\"static\", backend=\"ipex\", domain=\"nlp\")\n",
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config,\n",
    "    calibration_dataset=calibration_set,\n",
    "    save_directory=optimum_model_path,\n",
    "    batch_size=1,\n",
    ")\n",
    "tokenizer.save_pretrained(optimum_model_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8d9eb-354c-4d0f-8b68-d1cb3417dd29",
   "metadata": {},
   "source": [
    "#### Benchmark Optimum Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424628ae-091b-47bd-952f-96e1d6e7a1b2",
   "metadata": {},
   "source": [
    "This next class defines a wrapper around our SetFit model which plugs in our quantized model at inference (instead of the original model body)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf891fe1-03d4-44ce-a77e-9e86a9399f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimumSetFitModel:\n",
    "    def __init__(self, setfit_model, model_body):\n",
    "        model_body.tokenizer = setfit_model.model_body.tokenizer\n",
    "        self.model_body = model_body\n",
    "        self.model_head = setfit_model.model_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7215f",
   "metadata": {},
   "source": [
    "Time to run the performance benchmark on our optimized SetFit model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941e895-72e2-43e6-a3f4-9af39a5f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import IPEXModel\n",
    "\n",
    "optimum_model = IPEXModel.from_pretrained(optimum_model_path)\n",
    "optimum_setfit_model = OptimumSetFitModel(setfit_model, model_body=optimum_model)\n",
    "\n",
    "pb = PerformanceBenchmark(\n",
    "    model=optimum_setfit_model,\n",
    "    dataset=test_dataset,\n",
    "    optim_type=f\"bge-small (optimum-int8)\",\n",
    "    model_path=optimum_model_path,\n",
    "    autocast_dtype=torch.bfloat16,\n",
    ")\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6b2998-6605-4e42-9fb5-247c5eee750f",
   "metadata": {},
   "source": [
    "#### Plot Latencies and Compute Latency Speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8jpZ3gdWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8jpZ3gdWRn9",
    "outputId": "8d31c81a-67e4-4074-cf35-9f56d6dcdd20"
   },
   "outputs": [],
   "source": [
    "plot_metrics(perf_metrics)\n",
    "speedup = perf_metrics['bge-small (transformers)']['time_avg_ms'] / perf_metrics['bge-small (optimum-int8)']['time_avg_ms']\n",
    "print(f\"Latency speedup for 'bge-small (optimum-int8) vs bge-small (transformers)': {speedup:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48568c7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### After optimizing, inference with batch size 1 is 3.4x faster than before, with minimal drop in accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6872f-1765-48a5-beba-a989555b5461",
   "metadata": {},
   "source": [
    "#### Throughput Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e043497-89c0-44a7-b08f-1f8012cd9fba",
   "metadata": {},
   "source": [
    "Let's look now at the throughput, which is the number of samples the model can predict per second.\\\n",
    "We'll plot this value for our optimized and baseline models, as a function of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701770a-d643-4f32-bd56-2574ba7dc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_throughputs(perf_metrics):\n",
    "    plt.figure(figsize=(10, 6), dpi=120)\n",
    "    maximums = []\n",
    "    for i, (label, metric) in enumerate(perf_metrics.items()):\n",
    "        xs = np.arange(len(metric[\"batch_sizes\"]))\n",
    "        y_mean, y_std = np.array(metric[\"throughputs_avg\"]), np.array(metric[\"throughputs_std\"])\n",
    "        maximums.append((np.argmax(y_mean), max(y_mean)))\n",
    "        plt.plot(xs, y_mean, label=label, alpha=0.8)\n",
    "        plt.fill_between(xs, y_mean - y_std, y_mean + y_std, alpha=0.2)\n",
    "        for x, y in zip(xs, y_mean):\n",
    "            plt.text(x, y - 40 * (i % 2), f'{y:.1f}', ha='right', va='bottom', fontsize=7, color=\"black\")\n",
    "\n",
    "    plt.suptitle(\"Throughput vs Batch Size\")\n",
    "    plt.title('\\nBAAI/bge-small-en-v1.5 SetFit model trained on SetFit/sst2', fontsize=8)\n",
    "    plt.ylabel(\"Sentences / Second\")\n",
    "    plt.xlabel(\"Batch Size\")\n",
    "    plt.xticks(xs, pb.batch_sizes[:len(xs)])\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add Speedup Arrow\n",
    "    max_opt_thoughput = max(perf_metrics['bge-small (optimum-int8)']['throughputs_avg'])\n",
    "    max_transformers_thoughput = max(perf_metrics['bge-small (transformers)']['throughputs_avg'])\n",
    "    speedup =  max_opt_thoughput / max_transformers_thoughput\n",
    "    arrow_kwargs = dict(head_width = 0.6, width=0.15, head_length=60, color=\"green\", alpha=0.6)\n",
    "    plt.arrow(6.3, 200, 0, 500, **arrow_kwargs)\n",
    "    plt.text(6.5, 340, f'Max\\nThroughput\\nSpeedup={speedup:.1f}x', color=\"green\", fontsize=14)\n",
    "    \n",
    "    plt.savefig(\"throughput_vs_batch_size.png\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_throughputs(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6ab3-0d71-453a-9af3-67f8947fa6ce",
   "metadata": {},
   "source": [
    "#### The throughput is where our optimization shines - the optimized model achieves 7.8x speedup in max throughput!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "setfit",
   "language": "python",
   "name": "setfit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
