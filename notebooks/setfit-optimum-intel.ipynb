{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76571396-8f54-40ed-9e81-6c7531e6eaee",
   "metadata": {
    "id": "76571396-8f54-40ed-9e81-6c7531e6eaee"
   },
   "source": [
    "# Accelerate SetFit Models with [`optimum-intel`](https://github.com/huggingface/optimum-intel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733f283-4736-4d9d-854e-5582280b074d",
   "metadata": {},
   "source": [
    "In this notebook, we'll show how you can use the ðŸ¤— `optimum-intel` package to speed up your SetFit models on **Intel CPUs**.\\\n",
    "In our experiments, we will be using a fine-tuned SetFit model based on `bge-small-en-v1.5` sentence transformer model and observe our performance increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d874cea-d8d1-4fdf-b374-7772811fe7b7",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbde933-a2f1-4159-9b93-cfcf73f28c79",
   "metadata": {},
   "source": [
    "* Linux OS\n",
    "* Python 3.9+\n",
    "* 3rd 4th or 5th Generation Intel Xeon Processor (supports [bfloat16](https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-on-xeon-processors-with-bfloat16.html))\n",
    "\n",
    "Since Google Colab doesn't meet the hardware requirement, it is incompatible with this notebook and will not see speedups from our optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef66495-05e4-4b3f-86fb-6311b1d176c9",
   "metadata": {},
   "source": [
    "Please run the following to ensure you meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d9247a-b053-46af-89f6-e57067ecb8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements  met.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "is_python_ver_ok = sys.version_info.major == 3 and sys.version_info.minor >=9\n",
    "cpu_info = !lscpu\n",
    "is_cpu_ok = \"intel\" in cpu_info.grep(\"vendor\")[0].lower() and \"bf16\" in cpu_info.grep(\"flags\")[0]\n",
    "\n",
    "if not is_python_ver_ok:\n",
    "    print(f\"This notebook requires Python 3.9+ but your version is {sys.version.split()[0]}.\")\n",
    "\n",
    "if not is_cpu_ok:\n",
    "    print(f\"Your CPU does not support bfloat16.\")\n",
    "\n",
    "print(f\"Requirements {'' if is_python_ver_ok and is_cpu_ok else 'not'} met.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051b3ace-ef4f-4344-8255-13b11c8e04c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test with less cores, update following markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4099797-55d8-492a-94d4-4d815e72aeb8",
   "metadata": {},
   "source": [
    "### Reproducibility Note\n",
    "To reproduce the maximum speeds reported in this notebook, please launch it locally, using `numactl`.\\\n",
    "`numactl` enables gaining better control on which resources you're running. It's best to run on a single socket (`-m 0`):\\\n",
    "`OMP_NUM_THREADS=<NUM_THREADS> numactl -C <CORES_RANGE> -m 0 jupyter notebook`\n",
    "\n",
    "For example, if cores 0-59 are available, you can run with one thread per core like so:\\\n",
    "`OMP_NUM_THREADS=60 numactl -C 0-59 -m 0 jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc2a85",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Install packages required for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839bd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U setfit evaluate matplotlib -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72d255-ab98-4cc3-878b-6a60e08ccf6d",
   "metadata": {},
   "source": [
    "## Install `optimum-intel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cefb9e-67b8-40d2-9448-5cb54527555f",
   "metadata": {},
   "source": [
    "Install [`optimum-intel`](https://github.com/huggingface/optimum-intel) along with INC and IPEX backends which will be used in our optimization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9466ee2-5791-45b4-b69d-33bb5a82f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade-strategy eager optimum[neural-compressor] -qqq\n",
    "!python -m pip install intel-extension-for-pytorch -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2493cd04-812f-461b-bb26-9225d76c2774",
   "metadata": {},
   "source": [
    "## 1. Benchmark SetFit using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a090c0-a065-4b3f-b803-5e518c73299f",
   "metadata": {},
   "source": [
    "Define the infrastructure for conducting latency, throughput and accuracy benchmarks, as well as a plotting function for the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55756fec-fc22-4590-84d7-2f3df37b9256",
   "metadata": {
    "id": "55756fec-fc22-4590-84d7-2f3df37b9256"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import os\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def get_dir_size(path):\n",
    "    ps = subprocess.Popen(('du','-hk', path), stdout=subprocess.PIPE)\n",
    "    output = subprocess.check_output((\"awk\", \"{print $1/1024,$2}\"), stdin=ps.stdout)\n",
    "    ps.wait()\n",
    "    return float(output.split()[0].decode('utf-8'))\n",
    "\n",
    "BATCH_SIZES = [4, 16, 64, 1024, 2048, 8192]\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, model, dataset, optim_type, batch_sizes, metric=\"accuracy\", model_path=None, enable_autocast=False):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        self.metric = evaluate.load(metric)\n",
    "        self.model_path = model_path\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.enable_autocast = enable_autocast\n",
    "        \n",
    "    def model_predict(self, query):\n",
    "        with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "            return self.model(query)\n",
    "\n",
    "    def compute_accuracy(self):\n",
    "        with torch.no_grad(), torch.cpu.amp.autocast(enabled=self.enable_autocast):\n",
    "            preds = self.model.predict(self.dataset[\"text\"])\n",
    "\n",
    "        labels = self.dataset[\"label\"]\n",
    "        accuracy = self.metric.compute(predictions=preds, references=labels)\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy\n",
    "\n",
    "    def compute_size(self):\n",
    "        if self.model_path is None:\n",
    "            state_dict = self.model.model_body.state_dict()\n",
    "            tmp_path = Path(\"model.pt\")\n",
    "            torch.save(state_dict, tmp_path)\n",
    "            size_mb = get_dir_size(tmp_path)\n",
    "            # Delete temporary file\n",
    "            tmp_path.unlink()\n",
    "        else:\n",
    "            size_mb = get_dir_size(self.model_path)\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "\n",
    "    def compute_latency(self, query=\"that loves its characters and communicates something rather beautiful about human nature\"):\n",
    "        latencies = []\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            self.model_predict([query])\n",
    "        # Timed run\n",
    "        for _ in range(200):\n",
    "            start_time = perf_counter()\n",
    "            self.model_predict([query])\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        # Compute run statistics\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(rf\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "    def compute_throughput(self):\n",
    "        num_batches = 20\n",
    "        num_warmup_batches = 10\n",
    "        # Replicate the dataset examples to accomodate the largest batch size\n",
    "        max_batch_size = max(2048, max(self.batch_sizes))\n",
    "        replications = int((max_batch_size * (num_batches + 1)) / len(self.dataset))\n",
    "        replicated_ds = (self.dataset[\"text\"] * replications)[:max_batch_size * num_batches]\n",
    "\n",
    "        res = defaultdict(list)\n",
    "        res[\"batch_sizes\"] = self.batch_sizes\n",
    "        \n",
    "        for batch_size in self.batch_sizes:\n",
    "            throughputs = []\n",
    "            # Warmup\n",
    "            dataloader = iter(DataLoader(replicated_ds, batch_size=batch_size))\n",
    "            for _ in range(num_warmup_batches):\n",
    "                self.model_predict(next(dataloader))\n",
    "            # Timed run\n",
    "            dataloader = iter(DataLoader(replicated_ds, batch_size=batch_size))\n",
    "            for _ in tqdm(range(num_batches), file=sys.stdout):\n",
    "                start_time = perf_counter()\n",
    "                self.model_predict(next(dataloader))\n",
    "                total_time =  perf_counter() - start_time\n",
    "                throughputs.append(batch_size / total_time)\n",
    "            # Compute run statistics\n",
    "            res[\"throughputs_avg\"].append(np.mean(throughputs))\n",
    "            res[\"throughputs_std\"].append(np.std(throughputs))\n",
    "            print(rf\"Average throughput, {batch_size=} (samples/sec): {res['throughputs_avg'][-1]:.2f} +\\- {res['throughputs_std'][-1]:.2f}\")\n",
    "        return res\n",
    "        \n",
    "    def run_benchmark(self):\n",
    "        all_metrics = {}\n",
    "        for run_metric in self.compute_size, self.compute_accuracy, self.compute_latency, self.compute_throughput:\n",
    "            all_metrics |= run_metric()\n",
    "        return {self.optim_type: all_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878768f-9ec4-4675-ab20-0f8bca7bbe37",
   "metadata": {},
   "source": [
    "Let's add a function to plot our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99110d37-1aa1-4373-8517-f1f8acbbc9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(perf_metrics):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
    "    plt.figure(figsize=(6, 4), dpi=100)\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        plt.errorbar(\n",
    "            df_opt[\"time_avg_ms\"],\n",
    "            df_opt[\"accuracy\"] * 100,\n",
    "            xerr=df_opt[\"time_std_ms\"],\n",
    "            fmt=\"o\",\n",
    "            alpha=0.5,\n",
    "            ms=df_opt[\"size_mb\"] / 15,\n",
    "            label=idx,\n",
    "            capsize=5,\n",
    "            capthick=1,\n",
    "        )\n",
    "    legend = plt.legend(loc=\"lower right\")\n",
    "    plt.ylim(63, 95)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = max([metrics[\"time_avg_ms\"] for metrics in perf_metrics.values()]) * 1.3\n",
    "    plt.xlim(0, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency with batch_size=1 (ms)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159ea96-f604-478c-b6a2-bf4cc5c57a5a",
   "metadata": {},
   "source": [
    "Load the dataset for running evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867a9450-d8f0-4697-b3b4-a5ddf6b04906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b47c9-7076-4510-8c1e-5c17445bae34",
   "metadata": {},
   "source": [
    "We'll now run the benchmark with the standard PyTorch backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "u-w99Y2qW4lU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-w99Y2qW4lU",
    "outputId": "57f0b8f7-6dad-4e90-c779-658a7de6e960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 127.32\n",
      "Accuracy on test set - 0.906\n",
      "Average latency (ms) - 9.01 +\\- 0.63\n",
      "len(self.dataset)=872\n",
      "replications=197\n",
      "len(replicated_ds)=163840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e8ceb84494980b0eaae60801ecdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=4 (samples/sec): 280.43 +\\- 38.51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973655bd8d9b4616aad9da4184651a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=16 (samples/sec): 462.75 +\\- 44.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c31da897ba940ebbca607a246a66183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=64 (samples/sec): 274.15 +\\- 27.04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294a35a9f17648a3b186633a4f1c85be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=1024 (samples/sec): 926.11 +\\- 149.02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dd6de66ff54aa4b9d30ebf62e08700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=2048 (samples/sec): 1184.56 +\\- 29.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4582a530ee904260a486ac5150115c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=8192 (samples/sec): 1283.19 +\\- 16.08\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "small_model = SetFitModel.from_pretrained(\"moshew/bge-small-en-v1.5_setfit-sst2-english\")\n",
    "pb = PerformanceBenchmark(small_model, test_dataset, \"bge-small (PyTorch)\", batch_sizes=BATCH_SIZES)\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AiPUhOCNWRny",
   "metadata": {
    "id": "AiPUhOCNWRny"
   },
   "source": [
    "## 2. Optimizing with [`optimum-intel`](https://github.com/huggingface/optimum-intel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60d191-db3b-49a3-a281-34ae9063cb25",
   "metadata": {},
   "source": [
    "ðŸ¤— Optimum Intel is the interface between the ðŸ¤— Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures.\n",
    "\n",
    "In order to optimize our SetFit model, we'll use Intel [Neural Compressor](https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html) (INC), part of `optimum-intel`. \n",
    "We'll use INC to quantize the model body. This will compress the model size and result in faster inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0557f9-8193-4059-8056-bfc50c00deef",
   "metadata": {},
   "source": [
    "First, let's load the Qasper calibration dataset which we'll use for the quantization process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8466be-f28d-4303-8511-15798806a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_qasper_calibration_set(sample_size) -> Dataset:\n",
    "    train_set = load_dataset(\"allenai/qasper\")[\"train\"]\n",
    "    random.seed(666)\n",
    "    random_samples = random.sample(range(len(train_set)), sample_size)\n",
    "    random_queries = [random.sample(train_set[x][\"qas\"][\"question\"], 1)[0] for x in random_samples]\n",
    "    random_abstracts = [train_set[x][\"abstract\"] for x in random_samples]\n",
    "    samples = random.sample(random_queries + random_abstracts, sample_size)\n",
    "    random.shuffle(samples)\n",
    "    def gen():\n",
    "        for s in samples:\n",
    "            yield {\"text\": s}\n",
    "    return Dataset.from_generator(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b90c6ba-8f6e-4899-b404-ecebc0f7a78a",
   "metadata": {},
   "source": [
    "Define the desired quantization process using `optimum.intel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7739dc-2a56-4960-8f56-7f3f74991fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optimum.intel\n",
    "from neural_compressor.config import PostTrainingQuantConfig\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "def quantize(model_name, output_path, calibration_set):\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=512, truncation=True)\n",
    "\n",
    "    vectorized_ds = calibration_set.map(preprocess_function, num_proc=10)\n",
    "    vectorized_ds = vectorized_ds.remove_columns([\"text\"])\n",
    "\n",
    "    quantizer = optimum.intel.INCQuantizer.from_pretrained(model)\n",
    "    quantization_config = PostTrainingQuantConfig(approach=\"static\", backend=\"ipex\", domain=\"nlp\")\n",
    "    quantizer.quantize(\n",
    "        quantization_config=quantization_config,\n",
    "        calibration_dataset=vectorized_ds,\n",
    "        save_directory=output_path,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    \n",
    "model_name = \"moshew/bge-small-en-v1.5_setfit-sst2-english\"\n",
    "calibration_set = load_qasper_calibration_set(sample_size=100)\n",
    "optimum_model_path = f\"/tmp/{model_name}_opt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f935e3-d09a-4f44-87a0-f773b83e569a",
   "metadata": {},
   "source": [
    "Quantize our SetFit model using `optimum-intel` on 100 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508125d2-7bc4-4a6a-9b9d-1243ac0ff002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/common/criterion.py:430: DeprecationWarning: Call to deprecated function (or staticmethod) criterion_registry. -- Deprecated since version 2.0.\n",
      "  @criterion_registry(\"KnowledgeDistillationLoss\", \"pytorch\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/common/criterion.py:963: DeprecationWarning: Call to deprecated function (or staticmethod) criterion_registry. -- Deprecated since version 2.0.\n",
      "  @criterion_registry(\"IntermediateLayersKnowledgeDistillationLoss\", \"pytorch\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/common/optimizer.py:116: DeprecationWarning: Call to deprecated function (or staticmethod) optimizer_registry. -- Deprecated since version 2.0.\n",
      "  @optimizer_registry(\"SGD\", \"tensorflow\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/common/optimizer.py:143: DeprecationWarning: Call to deprecated function (or staticmethod) optimizer_registry. -- Deprecated since version 2.0.\n",
      "  @optimizer_registry(\"AdamW\", \"tensorflow\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/common/optimizer.py:177: DeprecationWarning: Call to deprecated function (or staticmethod) optimizer_registry. -- Deprecated since version 2.0.\n",
      "  @optimizer_registry(\"SGD\", \"pytorch\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/strategy.py:73: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class TuneStrategy(object):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/bayesian.py:35: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class BayesianTuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/auto_mixed_precision.py:33: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class AutoMixedPrecisionTuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/mse_v2.py:32: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class MSE_V2TuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/mse.py:33: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class MSETuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/basic.py:32: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class BasicTuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/exhaustive.py:26: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class ExhaustiveTuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/strategy/random.py:29: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class RandomTuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py:48: DeprecationWarning: Call to deprecated function (or staticmethod) pattern_registry. -- Deprecated since version 2.0.\n",
      "  @pattern_registry(pattern_type=\"tile_pattern_1x1\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py:58: DeprecationWarning: Call to deprecated function (or staticmethod) pattern_registry. -- Deprecated since version 2.0.\n",
      "  @pattern_registry(pattern_type=\"tile_pattern_2x2\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py:68: DeprecationWarning: Call to deprecated function (or staticmethod) pattern_registry. -- Deprecated since version 2.0.\n",
      "  @pattern_registry(pattern_type=\"tile_pattern_1x16\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py:78: DeprecationWarning: Call to deprecated function (or staticmethod) pattern_registry. -- Deprecated since version 2.0.\n",
      "  @pattern_registry(pattern_type=\"tile_pattern_4x1\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruning_recipes/patterns/tile_pattern.py:88: DeprecationWarning: Call to deprecated function (or staticmethod) pattern_registry. -- Deprecated since version 2.0.\n",
      "  @pattern_registry(pattern_type=\"tile_pattern_1x2\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruning_recipes/patterns/__init__.py:29: DeprecationWarning: Call to deprecated class PATTERNS. -- Deprecated since version 2.0.\n",
      "  patterns = PATTERNS()\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruner_legacy/pattern_lock.py:25: DeprecationWarning: Call to deprecated function (or staticmethod) pruner_registry. -- Deprecated since version 2.0.\n",
      "  class PatternLockPruner(Pruner):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruner_legacy/gradient_sensitivity.py:32: DeprecationWarning: Call to deprecated function (or staticmethod) pruner_registry. -- Deprecated since version 2.0.\n",
      "  class GradientSensitivityPruner(Pruner):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruner_legacy/magnitude.py:29: DeprecationWarning: Call to deprecated function (or staticmethod) pruner_registry. -- Deprecated since version 2.0.\n",
      "  class BasicMagnitudePruner(Pruner):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/pruner_legacy/group_lasso.py:30: DeprecationWarning: Call to deprecated function (or staticmethod) pruner_registry. -- Deprecated since version 2.0.\n",
      "  class GroupLassoPruner(BasicMagnitudePruner):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:496: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"F1\", \"tensorflow, tensorflow_itex, pytorch, mxnet, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:592: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"Accuracy\", \"tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:709: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"Loss\", \"tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:756: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"MAE\", \"tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:815: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"RMSE\", \"tensorflow, tensorflow_itex, pytorch, mxnet, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:858: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"MSE\", \"tensorflow, tensorflow_itex, pytorch, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:918: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"topk\", \"tensorflow, tensorflow_itex\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:987: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"topk\", \"pytorch, mxnet, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1056: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"COCOmAPv2\", \"tensorflow, tensorflow_itex, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1218: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"mAP\", \"tensorflow, tensorflow_itex, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1375: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"COCOmAP\", \"tensorflow, tensorflow_itex, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1398: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"VOCmAP\", \"tensorflow, tensorflow_itex, onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1421: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"SquadF1\", \"tensorflow, tensorflow_itex\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1464: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"mIOU\", \"tensorflow, tensorflow_itex\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1517: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"GLUE\", \"onnxrt_qlinearops, onnxrt_integerops\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/metric.py:1582: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"ROC\", \"pytorch\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/bleu.py:66: DeprecationWarning: Call to deprecated class UnicodeRegex. -- Deprecated since version 2.0.\n",
      "  uregex = UnicodeRegex()\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/metric/bleu.py:90: DeprecationWarning: Call to deprecated function (or staticmethod) metric_registry. -- Deprecated since version 2.0.\n",
      "  @metric_registry(\"BLEU\", \"tensorflow, tensorflow_itex\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/nas/basic_nas.py:34: DeprecationWarning: Call to deprecated function (or staticmethod) nas_registry. -- Deprecated since version 2.0.\n",
      "  @nas_registry(\"Basic\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/nas/dynas.py:32: DeprecationWarning: Call to deprecated function (or staticmethod) nas_registry. -- Deprecated since version 2.0.\n",
      "  @nas_registry(\"DyNAS\")\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/contrib/strategy/sigopt.py:34: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class SigOptTuneStrategy(TuneStrategy):\n",
      "/storage/sdp/anaconda3/envs/dk-setfit/lib/python3.9/site-packages/neural_compressor/experimental/contrib/strategy/tpe.py:44: DeprecationWarning: Call to deprecated function (or staticmethod) strategy_registry. -- Deprecated since version 2.0.\n",
      "  class TpeTuneStrategy(TuneStrategy):\n",
      "ONNX export is no supported for model with quantized embeddings\n",
      "2024-02-01 06:41:20 [INFO] Start auto tuning.\n",
      "2024-02-01 06:41:20 [INFO] Execute the tuning process due to detect the evaluation function.\n",
      "2024-02-01 06:41:20 [INFO] Adaptor has 5 recipes.\n",
      "2024-02-01 06:41:20 [INFO] 0 recipes specified by user.\n",
      "2024-02-01 06:41:20 [INFO] 3 recipes require future tuning.\n",
      "2024-02-01 06:41:21 [WARNING] Fail to remove /home/sdp/dkorat/setfit/notebooks/nc_workspace/2024-02-01_06-41-13/ipex_config_tmp.json.\n",
      "2024-02-01 06:41:21 [INFO] *** Initialize auto tuning\n",
      "2024-02-01 06:41:21 [INFO] {\n",
      "2024-02-01 06:41:21 [INFO]     'PostTrainingQuantConfig': {\n",
      "2024-02-01 06:41:21 [INFO]         'AccuracyCriterion': {\n",
      "2024-02-01 06:41:21 [INFO]             'criterion': 'relative',\n",
      "2024-02-01 06:41:21 [INFO]             'higher_is_better': True,\n",
      "2024-02-01 06:41:21 [INFO]             'tolerable_loss': 0.01,\n",
      "2024-02-01 06:41:21 [INFO]             'absolute': None,\n",
      "2024-02-01 06:41:21 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7f8728e7bfd0>>,\n",
      "2024-02-01 06:41:21 [INFO]             'relative': 0.01\n",
      "2024-02-01 06:41:21 [INFO]         },\n",
      "2024-02-01 06:41:21 [INFO]         'approach': 'post_training_static_quant',\n",
      "2024-02-01 06:41:21 [INFO]         'backend': 'ipex',\n",
      "2024-02-01 06:41:21 [INFO]         'calibration_sampling_size': [\n",
      "2024-02-01 06:41:21 [INFO]             100\n",
      "2024-02-01 06:41:21 [INFO]         ],\n",
      "2024-02-01 06:41:21 [INFO]         'device': 'cpu',\n",
      "2024-02-01 06:41:21 [INFO]         'diagnosis': False,\n",
      "2024-02-01 06:41:21 [INFO]         'domain': 'nlp',\n",
      "2024-02-01 06:41:21 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2024-02-01 06:41:21 [INFO]         'excluded_precisions': [\n",
      "2024-02-01 06:41:21 [INFO]         ],\n",
      "2024-02-01 06:41:21 [INFO]         'framework': 'pytorch_ipex',\n",
      "2024-02-01 06:41:21 [INFO]         'inputs': [\n",
      "2024-02-01 06:41:21 [INFO]         ],\n",
      "2024-02-01 06:41:21 [INFO]         'model_name': '',\n",
      "2024-02-01 06:41:21 [INFO]         'ni_workload_name': 'quantization',\n",
      "2024-02-01 06:41:21 [INFO]         'op_name_dict': None,\n",
      "2024-02-01 06:41:21 [INFO]         'op_type_dict': None,\n",
      "2024-02-01 06:41:21 [INFO]         'outputs': [\n",
      "2024-02-01 06:41:21 [INFO]         ],\n",
      "2024-02-01 06:41:21 [INFO]         'quant_format': 'default',\n",
      "2024-02-01 06:41:21 [INFO]         'quant_level': 'auto',\n",
      "2024-02-01 06:41:21 [INFO]         'recipes': {\n",
      "2024-02-01 06:41:21 [INFO]             'smooth_quant': False,\n",
      "2024-02-01 06:41:21 [INFO]             'smooth_quant_args': {\n",
      "2024-02-01 06:41:21 [INFO]             },\n",
      "2024-02-01 06:41:21 [INFO]             'layer_wise_quant': False,\n",
      "2024-02-01 06:41:21 [INFO]             'layer_wise_quant_args': {\n",
      "2024-02-01 06:41:21 [INFO]             },\n",
      "2024-02-01 06:41:21 [INFO]             'fast_bias_correction': False,\n",
      "2024-02-01 06:41:21 [INFO]             'weight_correction': False,\n",
      "2024-02-01 06:41:21 [INFO]             'gemm_to_matmul': True,\n",
      "2024-02-01 06:41:21 [INFO]             'graph_optimization_level': None,\n",
      "2024-02-01 06:41:21 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2024-02-01 06:41:21 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2024-02-01 06:41:21 [INFO]             'pre_post_process_quantization': True,\n",
      "2024-02-01 06:41:21 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2024-02-01 06:41:21 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2024-02-01 06:41:21 [INFO]             ],\n",
      "2024-02-01 06:41:21 [INFO]             'dedicated_qdq_pair': False,\n",
      "2024-02-01 06:41:21 [INFO]             'rtn_args': {\n",
      "2024-02-01 06:41:21 [INFO]             },\n",
      "2024-02-01 06:41:21 [INFO]             'awq_args': {\n",
      "2024-02-01 06:41:21 [INFO]             },\n",
      "2024-02-01 06:41:21 [INFO]             'gptq_args': {\n",
      "2024-02-01 06:41:21 [INFO]             },\n",
      "2024-02-01 06:41:21 [INFO]             'teq_args': {\n",
      "2024-02-01 06:41:21 [INFO]             }\n",
      "2024-02-01 06:41:21 [INFO]         },\n",
      "2024-02-01 06:41:21 [INFO]         'reduce_range': None,\n",
      "2024-02-01 06:41:21 [INFO]         'TuningCriterion': {\n",
      "2024-02-01 06:41:21 [INFO]             'max_trials': 100,\n",
      "2024-02-01 06:41:21 [INFO]             'objective': [\n",
      "2024-02-01 06:41:21 [INFO]                 'performance'\n",
      "2024-02-01 06:41:21 [INFO]             ],\n",
      "2024-02-01 06:41:21 [INFO]             'strategy': 'basic',\n",
      "2024-02-01 06:41:21 [INFO]             'strategy_kwargs': None,\n",
      "2024-02-01 06:41:21 [INFO]             'timeout': 0\n",
      "2024-02-01 06:41:21 [INFO]         },\n",
      "2024-02-01 06:41:21 [INFO]         'use_bf16': True\n",
      "2024-02-01 06:41:21 [INFO]     }\n",
      "2024-02-01 06:41:21 [INFO] }\n",
      "2024-02-01 06:41:21 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2024-02-01 06:41:21 [INFO]  Found 12 blocks\n",
      "2024-02-01 06:41:21 [INFO] Attention Blocks: 12\n",
      "2024-02-01 06:41:21 [INFO] FFN Blocks: 12\n",
      "2024-02-01 06:41:21 [INFO] Attention Blocks : \n",
      "2024-02-01 06:41:21 [INFO] [['encoder.layer.0.attention.self.query', 'encoder.layer.0.attention.self.key', 'encoder.layer.0.attention.self.value', 'encoder.layer.0.attention.output.dense'], ['encoder.layer.1.attention.self.query', 'encoder.layer.1.attention.self.key', 'encoder.layer.1.attention.self.value', 'encoder.layer.1.attention.output.dense'], ['encoder.layer.2.attention.self.query', 'encoder.layer.2.attention.self.key', 'encoder.layer.2.attention.self.value', 'encoder.layer.2.attention.output.dense'], ['encoder.layer.3.attention.self.query', 'encoder.layer.3.attention.self.key', 'encoder.layer.3.attention.self.value', 'encoder.layer.3.attention.output.dense'], ['encoder.layer.4.attention.self.query', 'encoder.layer.4.attention.self.key', 'encoder.layer.4.attention.self.value', 'encoder.layer.4.attention.output.dense'], ['encoder.layer.5.attention.self.query', 'encoder.layer.5.attention.self.key', 'encoder.layer.5.attention.self.value', 'encoder.layer.5.attention.output.dense'], ['encoder.layer.6.attention.self.query', 'encoder.layer.6.attention.self.key', 'encoder.layer.6.attention.self.value', 'encoder.layer.6.attention.output.dense'], ['encoder.layer.7.attention.self.query', 'encoder.layer.7.attention.self.key', 'encoder.layer.7.attention.self.value', 'encoder.layer.7.attention.output.dense'], ['encoder.layer.8.attention.self.query', 'encoder.layer.8.attention.self.key', 'encoder.layer.8.attention.self.value', 'encoder.layer.8.attention.output.dense'], ['encoder.layer.9.attention.self.query', 'encoder.layer.9.attention.self.key', 'encoder.layer.9.attention.self.value', 'encoder.layer.9.attention.output.dense'], ['encoder.layer.10.attention.self.query', 'encoder.layer.10.attention.self.key', 'encoder.layer.10.attention.self.value', 'encoder.layer.10.attention.output.dense'], ['encoder.layer.11.attention.self.query', 'encoder.layer.11.attention.self.key', 'encoder.layer.11.attention.self.value', 'encoder.layer.11.attention.output.dense']]\n",
      "2024-02-01 06:41:21 [INFO] FFN Blocks : \n",
      "2024-02-01 06:41:21 [INFO] [['encoder.layer.0.intermediate.dense', 'encoder.layer.0.output.dense'], ['encoder.layer.1.intermediate.dense', 'encoder.layer.1.output.dense'], ['encoder.layer.2.intermediate.dense', 'encoder.layer.2.output.dense'], ['encoder.layer.3.intermediate.dense', 'encoder.layer.3.output.dense'], ['encoder.layer.4.intermediate.dense', 'encoder.layer.4.output.dense'], ['encoder.layer.5.intermediate.dense', 'encoder.layer.5.output.dense'], ['encoder.layer.6.intermediate.dense', 'encoder.layer.6.output.dense'], ['encoder.layer.7.intermediate.dense', 'encoder.layer.7.output.dense'], ['encoder.layer.8.intermediate.dense', 'encoder.layer.8.output.dense'], ['encoder.layer.9.intermediate.dense', 'encoder.layer.9.output.dense'], ['encoder.layer.10.intermediate.dense', 'encoder.layer.10.output.dense'], ['encoder.layer.11.intermediate.dense', 'encoder.layer.11.output.dense']]\n",
      "2024-02-01 06:41:21 [INFO] Pass query framework capability elapsed time: 498.25 ms\n",
      "2024-02-01 06:41:21 [INFO] Get FP32 model baseline.\n",
      "2024-02-01 06:41:21 [INFO] Save tuning history to /home/sdp/dkorat/setfit/notebooks/nc_workspace/2024-02-01_06-41-13/./history.snapshot.\n",
      "2024-02-01 06:41:22 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2024-02-01 06:41:22 [INFO] Quantize the model with default config.\n",
      "2024-02-01 06:41:30 [INFO] |******Mixed Precision Statistics******|\n",
      "2024-02-01 06:41:30 [INFO] +---------------+-----------+----------+\n",
      "2024-02-01 06:41:30 [INFO] |    Op Type    |   Total   |   INT8   |\n",
      "2024-02-01 06:41:30 [INFO] +---------------+-----------+----------+\n",
      "2024-02-01 06:41:30 [INFO] |     matmul    |     24    |    24    |\n",
      "2024-02-01 06:41:30 [INFO] |     Linear    |     25    |    25    |\n",
      "2024-02-01 06:41:30 [INFO] +---------------+-----------+----------+\n",
      "2024-02-01 06:41:30 [INFO] Pass quantize model elapsed time: 8430.1 ms\n",
      "2024-02-01 06:41:30 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
      "2024-02-01 06:41:30 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2024-02-01 06:41:30 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2024-02-01 06:41:30 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2024-02-01 06:41:30 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2024-02-01 06:41:30 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n",
      "2024-02-01 06:41:30 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n",
      "2024-02-01 06:41:30 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2024-02-01 06:41:30 [INFO] [Strategy] Found a model that meets the accuracy requirements.\n",
      "2024-02-01 06:41:30 [INFO] Save tuning history to /home/sdp/dkorat/setfit/notebooks/nc_workspace/2024-02-01_06-41-13/./history.snapshot.\n",
      "2024-02-01 06:41:30 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2024-02-01 06:41:30 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2024-02-01 06:41:30 [INFO] Save deploy yaml to /home/sdp/dkorat/setfit/notebooks/nc_workspace/2024-02-01_06-41-13/deploy.yaml\n",
      "Model weights saved to /tmp/moshew/bge-small-en-v1.5_setfit-sst2-english_opt/pytorch_model.bin\n",
      "Configuration saved in /tmp/moshew/bge-small-en-v1.5_setfit-sst2-english_opt/inc_config.json\n"
     ]
    }
   ],
   "source": [
    "quantize(model_name, output_path=optimum_model_path, calibration_set=calibration_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8071a",
   "metadata": {},
   "source": [
    "Define a SetFit model wrapper which replaces the standard model body with the optimized model body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "enaQpBF9WRn9",
   "metadata": {
    "id": "enaQpBF9WRn9"
   },
   "outputs": [],
   "source": [
    "from setfit.exporters.utils import mean_pooling\n",
    "\n",
    "class OptimumSetFitModel:\n",
    "    def __init__(self, inc_model, tokenizer, model_head):\n",
    "        self.optimum_model = inc_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_head = model_head\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        encoded_inputs = self.tokenizer(\n",
    "            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(self.optimum_model.device)\n",
    "\n",
    "        outputs = self.optimum_model(**encoded_inputs)\n",
    "        embeddings = mean_pooling(\n",
    "            outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"]\n",
    "        )\n",
    "        return self.model_head.predict(embeddings.cpu())\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70f73e",
   "metadata": {},
   "source": [
    "Load the optimized model and the test dataset, and perform some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "qRviEk2WWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRviEk2WWRn9",
    "outputId": "33f010a8-376e-4f0c-b21b-97fe25bf1a81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /tmp/moshew/bge-small-en-v1.5_setfit-sst2-english_opt/inc_config.json\n",
      "INCConfig {\n",
      "  \"distillation\": {},\n",
      "  \"neural_compressor_version\": \"2.4.1\",\n",
      "  \"optimum_version\": \"1.16.2\",\n",
      "  \"pruning\": {},\n",
      "  \"quantization\": {\n",
      "    \"dataset_num_samples\": 100,\n",
      "    \"is_static\": true\n",
      "  },\n",
      "  \"save_onnx_model\": false,\n",
      "  \"torch_version\": \"2.1.2\",\n",
      "  \"transformers_version\": \"4.37.2\"\n",
      "}\n",
      "\n",
      "intel_extension_for_pytorch version is 2.1.100+cpu\n",
      "2024-02-01 06:41:31,559 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: moshew/bge-small-en-v1.5_setfit-sst2-english\n",
      "2024-02-01 06:41:32,732 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(optimum_model_path, model_max_length=512)\n",
    "optimum_model = optimum.intel.INCModel.from_pretrained(optimum_model_path)\n",
    "model = SetFitModel.from_pretrained(\"moshew/bge-small-en-v1.5_setfit-sst2-english\")\n",
    "optimum_setfit_model = OptimumSetFitModel(optimum_model, tokenizer, model.model_head)\n",
    "\n",
    "# Reload test dataset\n",
    "test_dataset = load_dataset(\"SetFit/sst2\")[\"validation\"]\n",
    "\n",
    "# Perform inference\n",
    "optimum_setfit_model(test_dataset[\"text\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7215f",
   "metadata": {},
   "source": [
    "Time to run the performance benchmark on our optimized SetFit model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O8jpZ3gdWRn9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8jpZ3gdWRn9",
    "outputId": "8d31c81a-67e4-4074-cf35-9f56d6dcdd20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 44.65\n",
      "Accuracy on test set - 0.906\n",
      "Average latency (ms) - 2.98 +\\- 0.11\n",
      "len(self.dataset)=872\n",
      "replications=197\n",
      "len(replicated_ds)=163840\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63a52c5692e410ab6155f45937e2590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=4 (samples/sec): 493.48 +\\- 408.59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30cd03608084b06bd6610077e40e6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=16 (samples/sec): 1581.19 +\\- 734.33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7593974ee7fc41e3a96e5423f95b5969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=64 (samples/sec): 320.25 +\\- 113.43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c94c8635334363ab9c6de991125a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=1024 (samples/sec): 2053.36 +\\- 142.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f74c852478240f1a61855e3dee56fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average throughput, batch_size=2048 (samples/sec): 2404.24 +\\- 174.09\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94dd4a988f794fcc8a456dff65db6e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pb = PerformanceBenchmark(\n",
    "    optimum_setfit_model,\n",
    "    test_dataset,\n",
    "    \"bge-small (optimum-intel)\",\n",
    "    batch_sizes=BATCH_SIZES,\n",
    "    model_path=optimum_model_path,\n",
    "    enable_autocast=True\n",
    ")\n",
    "\n",
    "perf_metrics.update(pb.run_benchmark())\n",
    "plot_metrics(perf_metrics)\n",
    "\n",
    "speedup = perf_metrics['bge-small (PyTorch)']['time_avg_ms'] / perf_metrics['bge-small (optimum-intel)']['time_avg_ms']\n",
    "print(f\"Latency speedup for 'bge-small (optimum-intel)': {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48568c7b",
   "metadata": {},
   "source": [
    "#### After optimizing, inference is 3.34x faster than before, with no drop in accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6872f-1765-48a5-beba-a989555b5461",
   "metadata": {},
   "source": [
    "### Throughput Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e043497-89c0-44a7-b08f-1f8012cd9fba",
   "metadata": {},
   "source": [
    "Let's look now at the throughput, which is the number of samples the model can predict per second.\\\n",
    "We'll plot this value for our optimized and baseline models, as a function of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701770a-d643-4f32-bd56-2574ba7dc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2_throughputs(perf_metrics):\n",
    "    xs = np.array(BATCH_SIZES)\n",
    "    plt.figure(figsize=(10, 4), dpi=120)\n",
    "    for (label, metric), color in zip(perf_metrics.items(), [\"darkorange\", \"blue\"]):\n",
    "        y_mean, y_std = np.array(metric[\"throughputs_avg\"]), np.array(metric[\"throughputs_std\"])\n",
    "        plt.plot(xs, y_mean, label=label, color=color, alpha=0.8)\n",
    "        plt.fill_between(xs, y_mean - y_std, y_mean + y_std, color=color, alpha=0.2)\n",
    "\n",
    "        for x, y in zip(xs, y_mean):\n",
    "            plt.text(x, y, f'{y:.1f}', ha='right', va='bottom', fontsize=7, color=\"black\")\n",
    "            \n",
    "    plt.xscale('log')\n",
    "    plt.xticks(BATCH_SIZES, BATCH_SIZES)\n",
    "    plt.xlabel('Batch Size (log scale)')\n",
    "    plt.ylabel('Samples/Second')\n",
    "    plt.title('Throughput vs Batch Size')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "plot_2_throughputs(perf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c6ab3-0d71-453a-9af3-67f8947fa6ce",
   "metadata": {},
   "source": [
    "#### Similarly to the latency speedup, we can see that our optmization has resulted in 3x-4x throughput increase as well!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (new-dk-setfit)",
   "language": "python",
   "name": "dk-setfit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
